{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.11"
    },
    "colab": {
      "name": "GAN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AhmadArrabi/Cat-Generation-using-GANs/blob/main/GAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2dZnNyGFZpFo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7bda2d1a-56e0-40a4-d25a-82d2928072a9"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1dC9SwHNSojo"
      },
      "source": [
        "import os\n",
        "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
        "import numpy\n",
        "from PIL import Image\n",
        "import tensorflow as tf\n",
        "from tqdm import tqdm\n",
        "from tensorflow import keras"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iq0C5o81Utp4",
        "outputId": "d891cfdc-31f4-4f24-81dd-c178161f41ae"
      },
      "source": [
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pgaOrVPnSojr",
        "outputId": "a646f7b5-0015-47c5-f7b5-49533883fb72"
      },
      "source": [
        "dataset = keras.preprocessing.image_dataset_from_directory(\n",
        "    \"/content/drive/MyDrive/data/dataset/dataset3\",\n",
        "    label_mode=None,\n",
        "    image_size=(64, 64),\n",
        "    batch_size=128\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 4486 files belonging to 1 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vXTm1GootSAQ"
      },
      "source": [
        "#images = [file for file in os.listdir('/content/drive/MyDrive/data/dataset/dataset2') if file.endswith(('jpeg', 'png', 'jpg'))]\n",
        "#i = 0\n",
        "#for image in images:\n",
        "#    img = Image.open('/content/drive/MyDrive/data/dataset/dataset2/'+image)\n",
        "#    img.thumbnail((64,64))\n",
        "#    name = '/content/drive/MyDrive/data/dataset/dataset22/' + str(i) + '.png'\n",
        "#    img.save(name, optimize=True, quality=40)\n",
        "#    i = i+1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ndGnKt1GSojs"
      },
      "source": [
        "dataset = dataset.map(lambda x: x / 255.)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5BBnl8ctUaRm"
      },
      "source": [
        "------------------------------------------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ZNuOWq6UY8h",
        "outputId": "60a08356-343b-44e0-923d-9de7b6e09c9e"
      },
      "source": [
        "discriminator = keras.Sequential(\n",
        "    [\n",
        "     keras.Input(shape = (64,64,3)),\n",
        "     keras.layers.Conv2D(filters=128 ,kernel_size=5, strides=2, padding='same'),\n",
        "     keras.layers.LeakyReLU(0.2),\n",
        "     keras.layers.Conv2D(filters=256 ,kernel_size=5, strides=2, padding='same'),\n",
        "     keras.layers.LeakyReLU(0.2),\n",
        "     keras.layers.Conv2D(filters=512 ,kernel_size=5, strides=2, padding='same'),\n",
        "     keras.layers.LeakyReLU(0.2),\n",
        "     keras.layers.Conv2D(filters=1024 ,kernel_size=5, strides=2, padding='same'),\n",
        "     keras.layers.LeakyReLU(0.2),\n",
        "     keras.layers.BatchNormalization(),\n",
        "     keras.layers.Flatten(),\n",
        "     keras.layers.Dense(1, activation='sigmoid')\n",
        "    ]\n",
        ")\n",
        "print(discriminator.summary())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 32, 32, 128)       9728      \n",
            "_________________________________________________________________\n",
            "leaky_re_lu (LeakyReLU)      (None, 32, 32, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 16, 16, 256)       819456    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_1 (LeakyReLU)    (None, 16, 16, 256)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 8, 8, 512)         3277312   \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_2 (LeakyReLU)    (None, 8, 8, 512)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 4, 4, 1024)        13108224  \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_3 (LeakyReLU)    (None, 4, 4, 1024)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 4, 4, 1024)        4096      \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 16384)             0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1)                 16385     \n",
            "=================================================================\n",
            "Total params: 17,235,201\n",
            "Trainable params: 17,233,153\n",
            "Non-trainable params: 2,048\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "relT6i4bUY_o",
        "outputId": "a1018e80-08d4-4591-c61c-26bcdc777f5d"
      },
      "source": [
        "latent_dim = 128\n",
        "\n",
        "generator = keras.Sequential(\n",
        "    [\n",
        "     keras.layers.Input(shape=(latent_dim,)),\n",
        "     keras.layers.Dense(4*4*1024),\n",
        "     keras.layers.BatchNormalization(),\n",
        "     keras.layers.Reshape((4,4,1024)),\n",
        "     keras.layers.Conv2DTranspose(filters = 512, kernel_size=5, strides=2, padding='same'),\n",
        "     keras.layers.LeakyReLU(0.2),\n",
        "     keras.layers.Conv2DTranspose(filters = 256, kernel_size=5, strides=2, padding='same'),\n",
        "     keras.layers.LeakyReLU(0.2),\n",
        "     keras.layers.Conv2DTranspose(filters = 128, kernel_size=5, strides=2, padding='same'),\n",
        "     keras.layers.LeakyReLU(0.2),\n",
        "     keras.layers.Conv2DTranspose(filters = 3, kernel_size=5, strides=2, padding='same', activation='tanh'),\n",
        "    ]\n",
        ")\n",
        "print(generator.summary())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_1 (Dense)              (None, 16384)             2113536   \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 16384)             65536     \n",
            "_________________________________________________________________\n",
            "reshape (Reshape)            (None, 4, 4, 1024)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose (Conv2DTran (None, 8, 8, 512)         13107712  \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_4 (LeakyReLU)    (None, 8, 8, 512)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_1 (Conv2DTr (None, 16, 16, 256)       3277056   \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_5 (LeakyReLU)    (None, 16, 16, 256)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_2 (Conv2DTr (None, 32, 32, 128)       819328    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_6 (LeakyReLU)    (None, 32, 32, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_3 (Conv2DTr (None, 64, 64, 3)         9603      \n",
            "=================================================================\n",
            "Total params: 19,392,771\n",
            "Trainable params: 19,360,003\n",
            "Non-trainable params: 32,768\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ksS7rUCkUZGN"
      },
      "source": [
        "gen_opt = keras.optimizers.Adam(learning_rate=0.0002, beta_1=0.5)\n",
        "disc_opt = keras.optimizers.Adam(learning_rate=0.0002, beta_1=0.5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3jnAuDveUZJz"
      },
      "source": [
        "loss = keras.losses.BinaryCrossentropy()\n",
        "latent_dim = 128"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DE8Zz-zXUZMT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b937f919-8ee6-406b-88f1-ea9331102a89"
      },
      "source": [
        "with tf.device('/device:GPU:0'):\n",
        "  for epoch in range(100):\n",
        "    for idx, real in enumerate(tqdm(dataset)):\n",
        "      batch_size = real.shape[0]\n",
        "      random_latent_vector = tf.random.normal(shape=(batch_size, latent_dim))\n",
        "      fake = generator(random_latent_vector)\n",
        "\n",
        "      if idx % 100 == 0:\n",
        "        img = keras.preprocessing.image.array_to_img(fake[0])\n",
        "        img.save(\"/content/drive/MyDrive/data/dataset/generated3/part3/generated_img_%03d_%d.jpg\" % (epoch, idx))\n",
        "      \n",
        "      #Train Discriminator ylog(y') + (1-y)log(1-y') => ylog(D(x)) + (1-y)log(1-D(G(z)))\n",
        "      with tf.GradientTape() as disc_tape:\n",
        "        loss_disc_real = loss(tf.ones((batch_size,1)), discriminator(real))\n",
        "        loss_disc_fake = loss(tf.zeros((batch_size,1)), discriminator(fake))\n",
        "        loss_disc = (loss_disc_fake + loss_disc_real)\n",
        "      \n",
        "      grads = disc_tape.gradient(loss_disc, discriminator.trainable_weights)\n",
        "      disc_opt.apply_gradients(\n",
        "          zip(grads, discriminator.trainable_weights)\n",
        "      )\n",
        "      \n",
        "      #Train generator: log(D(G(z)))\n",
        "      with tf.GradientTape() as gen_tape:\n",
        "        fake = generator(random_latent_vector)\n",
        "        output = discriminator(fake)\n",
        "        loss_gen = loss(tf.ones(batch_size,1), output)\n",
        "      \n",
        "      grads = gen_tape.gradient(loss_gen, generator.trainable_weights)\n",
        "      gen_opt.apply_gradients(\n",
        "          zip(grads, generator.trainable_weights)\n",
        "      )\n",
        "\n",
        "      #training on ~2600 images took 1h 48m\n",
        "      #training on ~1800 images took 1h 5esh m\n",
        "      #training on ~4600 images took 2h 20m"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 36/36 [02:21<00:00,  3.94s/it]\n",
            "100%|██████████| 36/36 [01:19<00:00,  2.20s/it]\n",
            "100%|██████████| 36/36 [01:21<00:00,  2.28s/it]\n",
            "100%|██████████| 36/36 [01:19<00:00,  2.21s/it]\n",
            "100%|██████████| 36/36 [01:21<00:00,  2.28s/it]\n",
            "100%|██████████| 36/36 [01:19<00:00,  2.21s/it]\n",
            "100%|██████████| 36/36 [01:19<00:00,  2.21s/it]\n",
            "100%|██████████| 36/36 [01:19<00:00,  2.21s/it]\n",
            "100%|██████████| 36/36 [01:21<00:00,  2.28s/it]\n",
            "100%|██████████| 36/36 [01:21<00:00,  2.28s/it]\n",
            "100%|██████████| 36/36 [01:19<00:00,  2.21s/it]\n",
            "100%|██████████| 36/36 [01:19<00:00,  2.21s/it]\n",
            "100%|██████████| 36/36 [01:21<00:00,  2.28s/it]\n",
            "100%|██████████| 36/36 [01:21<00:00,  2.28s/it]\n",
            "100%|██████████| 36/36 [01:19<00:00,  2.21s/it]\n",
            "100%|██████████| 36/36 [01:21<00:00,  2.28s/it]\n",
            "100%|██████████| 36/36 [01:21<00:00,  2.28s/it]\n",
            "100%|██████████| 36/36 [01:19<00:00,  2.21s/it]\n",
            "100%|██████████| 36/36 [01:19<00:00,  2.21s/it]\n",
            "100%|██████████| 36/36 [01:19<00:00,  2.21s/it]\n",
            "100%|██████████| 36/36 [01:21<00:00,  2.28s/it]\n",
            "100%|██████████| 36/36 [01:21<00:00,  2.28s/it]\n",
            "100%|██████████| 36/36 [01:19<00:00,  2.20s/it]\n",
            "100%|██████████| 36/36 [01:20<00:00,  2.24s/it]\n",
            "100%|██████████| 36/36 [01:21<00:00,  2.28s/it]\n",
            "100%|██████████| 36/36 [01:20<00:00,  2.24s/it]\n",
            "100%|██████████| 36/36 [01:20<00:00,  2.25s/it]\n",
            "100%|██████████| 36/36 [01:21<00:00,  2.28s/it]\n",
            "100%|██████████| 36/36 [01:20<00:00,  2.24s/it]\n",
            "100%|██████████| 36/36 [01:20<00:00,  2.24s/it]\n",
            "100%|██████████| 36/36 [01:20<00:00,  2.25s/it]\n",
            "100%|██████████| 36/36 [01:20<00:00,  2.24s/it]\n",
            "100%|██████████| 36/36 [01:21<00:00,  2.28s/it]\n",
            "100%|██████████| 36/36 [01:21<00:00,  2.28s/it]\n",
            "100%|██████████| 36/36 [01:20<00:00,  2.24s/it]\n",
            "100%|██████████| 36/36 [01:20<00:00,  2.24s/it]\n",
            "100%|██████████| 36/36 [01:20<00:00,  2.25s/it]\n",
            "100%|██████████| 36/36 [01:20<00:00,  2.24s/it]\n",
            "100%|██████████| 36/36 [01:21<00:00,  2.28s/it]\n",
            "100%|██████████| 36/36 [01:20<00:00,  2.24s/it]\n",
            "100%|██████████| 36/36 [01:20<00:00,  2.24s/it]\n",
            "100%|██████████| 36/36 [01:21<00:00,  2.28s/it]\n",
            "100%|██████████| 36/36 [01:21<00:00,  2.28s/it]\n",
            "100%|██████████| 36/36 [01:20<00:00,  2.24s/it]\n",
            "100%|██████████| 36/36 [01:21<00:00,  2.28s/it]\n",
            "100%|██████████| 36/36 [01:21<00:00,  2.28s/it]\n",
            "100%|██████████| 36/36 [01:20<00:00,  2.25s/it]\n",
            "100%|██████████| 36/36 [01:20<00:00,  2.24s/it]\n",
            "100%|██████████| 36/36 [01:20<00:00,  2.24s/it]\n",
            "100%|██████████| 36/36 [01:21<00:00,  2.28s/it]\n",
            "100%|██████████| 36/36 [01:20<00:00,  2.23s/it]\n",
            "100%|██████████| 36/36 [01:20<00:00,  2.24s/it]\n",
            "100%|██████████| 36/36 [01:20<00:00,  2.24s/it]\n",
            "100%|██████████| 36/36 [01:20<00:00,  2.24s/it]\n",
            "100%|██████████| 36/36 [01:20<00:00,  2.24s/it]\n",
            "100%|██████████| 36/36 [01:21<00:00,  2.28s/it]\n",
            "100%|██████████| 36/36 [01:20<00:00,  2.24s/it]\n",
            "100%|██████████| 36/36 [01:20<00:00,  2.24s/it]\n",
            "100%|██████████| 36/36 [01:20<00:00,  2.24s/it]\n",
            "100%|██████████| 36/36 [01:21<00:00,  2.28s/it]\n",
            "100%|██████████| 36/36 [01:21<00:00,  2.28s/it]\n",
            "100%|██████████| 36/36 [01:21<00:00,  2.28s/it]\n",
            "100%|██████████| 36/36 [01:20<00:00,  2.24s/it]\n",
            "100%|██████████| 36/36 [01:20<00:00,  2.24s/it]\n",
            "100%|██████████| 36/36 [01:20<00:00,  2.24s/it]\n",
            "100%|██████████| 36/36 [01:20<00:00,  2.24s/it]\n",
            "100%|██████████| 36/36 [01:20<00:00,  2.24s/it]\n",
            "100%|██████████| 36/36 [01:21<00:00,  2.28s/it]\n",
            "100%|██████████| 36/36 [01:21<00:00,  2.28s/it]\n",
            "100%|██████████| 36/36 [01:20<00:00,  2.24s/it]\n",
            "100%|██████████| 36/36 [01:19<00:00,  2.22s/it]\n",
            "100%|██████████| 36/36 [01:21<00:00,  2.28s/it]\n",
            "100%|██████████| 36/36 [01:21<00:00,  2.28s/it]\n",
            "100%|██████████| 36/36 [01:21<00:00,  2.28s/it]\n",
            "100%|██████████| 36/36 [01:21<00:00,  2.28s/it]\n",
            "100%|██████████| 36/36 [01:19<00:00,  2.21s/it]\n",
            "100%|██████████| 36/36 [01:21<00:00,  2.28s/it]\n",
            "100%|██████████| 36/36 [01:21<00:00,  2.28s/it]\n",
            "100%|██████████| 36/36 [01:19<00:00,  2.21s/it]\n",
            "100%|██████████| 36/36 [01:21<00:00,  2.28s/it]\n",
            "100%|██████████| 36/36 [01:21<00:00,  2.28s/it]\n",
            "100%|██████████| 36/36 [01:21<00:00,  2.28s/it]\n",
            "100%|██████████| 36/36 [01:21<00:00,  2.28s/it]\n",
            "100%|██████████| 36/36 [01:19<00:00,  2.20s/it]\n",
            "100%|██████████| 36/36 [01:19<00:00,  2.20s/it]\n",
            "100%|██████████| 36/36 [01:21<00:00,  2.28s/it]\n",
            "100%|██████████| 36/36 [01:21<00:00,  2.28s/it]\n",
            "100%|██████████| 36/36 [01:21<00:00,  2.28s/it]\n",
            "100%|██████████| 36/36 [01:19<00:00,  2.21s/it]\n",
            "100%|██████████| 36/36 [01:19<00:00,  2.21s/it]\n",
            "100%|██████████| 36/36 [01:21<00:00,  2.28s/it]\n",
            "100%|██████████| 36/36 [01:21<00:00,  2.28s/it]\n",
            "100%|██████████| 36/36 [01:21<00:00,  2.28s/it]\n",
            "100%|██████████| 36/36 [01:21<00:00,  2.28s/it]\n",
            "100%|██████████| 36/36 [01:21<00:00,  2.28s/it]\n",
            "100%|██████████| 36/36 [01:19<00:00,  2.21s/it]\n",
            "100%|██████████| 36/36 [01:21<00:00,  2.28s/it]\n",
            "100%|██████████| 36/36 [01:20<00:00,  2.22s/it]\n",
            "100%|██████████| 36/36 [01:19<00:00,  2.22s/it]\n",
            "100%|██████████| 36/36 [01:21<00:00,  2.28s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F4le3YZgHH0A",
        "outputId": "bde94518-29a9-42fe-cc01-a43a64371312"
      },
      "source": [
        "3print(generator.get_weights)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<bound method Model.get_weights of <keras.engine.sequential.Sequential object at 0x7fc56ab559d0>>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R1hAhV_FJpay"
      },
      "source": [
        "# Saving models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y8Q9qsRJHZOq",
        "outputId": "7e6af179-fcde-429a-dd49-22555c7493da"
      },
      "source": [
        "import os.path\n",
        "if os.path.isfile('/content/drive/MyDrive/data/models/generator.h5') is False:\n",
        "  generator.save('/content/drive/MyDrive/data/models/generator.h5')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wjS0AHyVH3XC",
        "outputId": "c63a5ce6-aa45-4c13-e6b5-ddb281b0c723"
      },
      "source": [
        "if os.path.isfile('/content/drive/MyDrive/data/models/dicriminator.h5') is False:\n",
        "  discriminator.save('/content/drive/MyDrive/data/models/dicriminator.h5')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Ji1ingnMHCi"
      },
      "source": [
        "## Loading Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gO3oFBhzJF7M",
        "outputId": "d077e5c3-9aec-4731-ac5b-b976cb992e41"
      },
      "source": [
        "from keras.models import load_model\n",
        "discriminator = load_model('/content/drive/MyDrive/data/models/dicriminator.h5')\n",
        "discriminator.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 32, 32, 128)       9728      \n",
            "_________________________________________________________________\n",
            "leaky_re_lu (LeakyReLU)      (None, 32, 32, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 16, 16, 256)       819456    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_1 (LeakyReLU)    (None, 16, 16, 256)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 8, 8, 512)         3277312   \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_2 (LeakyReLU)    (None, 8, 8, 512)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 4, 4, 1024)        13108224  \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_3 (LeakyReLU)    (None, 4, 4, 1024)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 4, 4, 1024)        4096      \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 16384)             0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1)                 16385     \n",
            "=================================================================\n",
            "Total params: 17,235,201\n",
            "Trainable params: 17,233,153\n",
            "Non-trainable params: 2,048\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EuvpOHblJd1D",
        "outputId": "c9f11d1a-54c6-4869-b84f-16cb60f5204d"
      },
      "source": [
        "generator = load_model('/content/drive/MyDrive/data/models/generator.h5')\n",
        "generator.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_1 (Dense)              (None, 16384)             2113536   \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 16384)             65536     \n",
            "_________________________________________________________________\n",
            "reshape (Reshape)            (None, 4, 4, 1024)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose (Conv2DTran (None, 8, 8, 512)         13107712  \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_4 (LeakyReLU)    (None, 8, 8, 512)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_1 (Conv2DTr (None, 16, 16, 256)       3277056   \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_5 (LeakyReLU)    (None, 16, 16, 256)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_2 (Conv2DTr (None, 32, 32, 128)       819328    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_6 (LeakyReLU)    (None, 32, 32, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_3 (Conv2DTr (None, 64, 64, 3)         9603      \n",
            "=================================================================\n",
            "Total params: 19,392,771\n",
            "Trainable params: 19,360,003\n",
            "Non-trainable params: 32,768\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "84_jP-htuCML"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}